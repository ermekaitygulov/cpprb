#+OPTIONS: ':nil -:nil ^:{} num:t toc:nil
#+AUTHOR: Hiroyuki Yamada
#+CREATOR: Emacs 26.1 (Org mode 9.1.14 + ox-hugo)
#+HUGO_WITH_LOCALE:
#+HUGO_FRONT_MATTER_FORMAT: toml
#+HUGO_LEVEL_OFFSET: 1
#+HUGO_PRESERVE_FILLING:
#+HUGO_DELETE_TRAILING_WS:
#+HUGO_SECTION: .
#+HUGO_BUNDLE:
#+HUGO_BASE_DIR: ./
#+HUGO_CODE_FENCE:
#+HUGO_USE_CODE_FOR_KBD:
#+HUGO_PREFER_HYPHEN_IN_TAGS:
#+HUGO_ALLOW_SPACES_IN_TAGS:
#+HUGO_AUTO_SET_LASTMOD:
#+HUGO_CUSTOM_FRONT_MATTER:
#+HUGO_BLACKFRIDAY:
#+HUGO_FRONT_MATTER_KEY_REPLACE:
#+HUGO_DATE_FORMAT: %Y-%m-%dT%T+09:00
#+HUGO_PAIRED_SHORTCODES:
#+HUGO_PANDOC_CITATIONS:
#+BIBLIOGRAPHY:
#+HUGO_ALIASES:
#+HUGO_AUDIO:
#+DATE: <2019-02-10 Sun>
#+DESCRIPTION:
#+HUGO_DRAFT:
#+HUGO_EXPIRYDATE:
#+HUGO_HEADLESS:
#+HUGO_IMAGES:
#+HUGO_ISCJKLANGUAGE:
#+KEYWORDS:
#+HUGO_LAYOUT:
#+HUGO_LASTMOD:
#+HUGO_LINKTITLE:
#+HUGO_LOCALE:
#+HUGO_MARKUP:
#+HUGO_MENU:
#+HUGO_MENU_OVERRIDE:
#+HUGO_OUTPUTS:
#+HUGO_PUBLISHDATE:
#+HUGO_SERIES:
#+HUGO_SLUG:
#+HUGO_TAGS:
#+HUGO_CATEGORIES:
#+HUGO_RESOURCES:
#+HUGO_TYPE:
#+HUGO_URL:
#+HUGO_VIDEOS:
#+HUGO_WEIGHT: auto

#+STARTUP: showall logdone

* Installation
:PROPERTIES:
:EXPORT_HUGO_SECTION*: installation
:END:

** DONE Step by step installation on macOS
CLOSED: [2020-01-17 Fri 21:09]
:PROPERTIES:
:EXPORT_FILE_NAME: install_on_macos
:END:

Since clang (libc++) has not implemented array specialization of
=std::shared_ptr= ([[http://libcxx.llvm.org/cxx1z_status.html][libc++ C++17 Status]], P0414R2 etc.), cpprb cannot be
compiled by clang.

Here we describe how to install cpprb on macOS using [[https://www.macports.org/][MacPorts]].

#+begin_src shell
sudo port selfupdate
sudo port install gcc9
sudo port select gcc mp-gcc9
git clone https://gitlab.com/ymd_h/cpprb.git
cd cpprb
CC=/opt/local/bin/g++ CXX=/opt/local/bin/g++ pip install .
#+end_src

* Features
:PROPERTIES:
:EXPORT_HUGO_SECTION*: features
:END:

** DONE Flexible Environment
CLOSED: [2019-11-08 Fri 05:58]
:PROPERTIES:
:EXPORT_FILE_NAME: flexible_environment
:END:

*** Overview

In ~cpprb~ version 8 and newer, you can store any number of
environments (aka. observation, action, etc.).

For example, you can add your special environments like
~next_next_obs~, ~second_reward~, and so on.

These environments can take multi-dimensional shape (e.g. ~3~,
~(4,4)~, ~(84,84,4)~), and any [[https://numpy.org/devdocs/user/basics.types.html][numpy data type]].


**** ~__init__~
In order to construct replay buffers, you need to specify the second
parameter of their constructor, ~env_dict~.

The ~env_dict~ is a ~dict~ whose keys are environment name and whose
values are ~dict~ describing their properties.

The following table is supported properties and their default values.

| key   | description                    | type                         | default value                                    |
|-------+--------------------------------+------------------------------+--------------------------------------------------|
| shape | shape (size of each dimension) | ~int~ or array like of ~int~ | ~1~                                              |
| dtype | data type                      | ~numpy.dtype~                | ~default_dtype~ in constructor or ~numpy.single~ |

**** ~add~
When ~add~ -ing environments to the replay buffer, you have to pass
them by keyword arguments (aka. ~key=value~ style). If your
environment name is not a syntactically valid identifier, you can
still create dictionary first, then unpack the dictionary by ~**~
operator (e.g. ~rb.add(**kwargs)~).

**** ~sample~
~sample~ returns ~dict~ with keys of environments' name and with
values of sampled ones.


*** Example Usage

#+begin_src python
from cpprb import ReplayBuffer
import numpy as np

buffer_size = 32

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (4,4)},
                               "act": {"shape": 1},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "next_next_obs": {"shape": (4,4)},
                               "done": {},
                               "my_important_info": {"dtype": {np.short}}})

rb.add(obs=np.zeros((4,4)),
       act=1.5,
       rew=0.0,
       next_obs=np.zeros((4,4)),
       next_next_obs=np.zeros((4,4)),
       done=0,
       my_important_info=2)
#+end_src
*** Notes
~priorities~, ~weights~, and ~indexes~ for ~PrioritizedReplayBuffer~
are special environments and are automatically set.


*** Technical Detail
Internally, these flexible environments are implemented with (cython
version of) ~numpy.ndarray~. They were implemented with C++ code in
older than version 8, which had trouble in flexibilities of data type
and the number of environment. (There was a dirty hack to put all
extra environments into ~act~ which was not treat specially.)


** DONE Multistep-add
CLOSED: [2019-11-10 Sun 14:08]
:PROPERTIES:
:EXPORT_FILE_NAME: multistep_add
:END:

*** Overview
In cpprb, you can add multistep environment to replay buffer simultaneously.


*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs": {"shape": 5},
                      "act": {"shape": 3},
                      "rew": {},
                      "next_obs": {"shape": 5},
                      "done": {}})

steps = 10

rb.get_stored_size() # -> 0


rb.add(obs=np.ones(steps,5),
       act=np.zeros(steps,3),
       rew=np.ones(steps),
       next_obs=np.ones(steps,5),
       done=np.zeros(steps))


rb.get_stored_size() # -> steps
#+end_src
*** Notes
The dimension for step must be 0th dimension

*** Technical Detail
The shapes for ~add~ for every environments are stored as
~add_shape=(-1,*env_shape)~ at constructor, s.t. ~env_shape~ is the environment
shape.

Only one environment (usually ~done~) is used to determine the step
size by reshaping to ~add_shape~.


** DONE Prioritized Experience Replay
CLOSED: [2019-11-10 Sun 13:26]
:PROPERTIES:
:EXPORT_FILE_NAME: PER
:END:

*** Overview
Prioritized experience replay was proposed by [[https://arxiv.org/abs/1511.05952][T. Schaul et. al.]], and
is widely used to speed up reinforcement learning (as far as I know).

Roughly speaking, mis-predicted observations will be learned more
frequently. To compensate distorted probability, weight of learning is
scaled to the opposite direction (cf. importance sampling).

In cpprb, ~PrioritizedReplayBuffer~ class implements these
functionalities with proportional base (instead of rank base)
priorities.


You can ~add~ ~priorities~ together with other environment. If no
~prioroties~ are passed, the stored maximum priority is used.


The ~dict~ returned by ~sample~ also has special key-values of
~indexes~ and ~weights~. The ~indexes~ are intended to be passed to
~update_priorities~ to update their priorities after comparison with new
prediction.


~PrioritizedReplayBuffer~ has hyperparameters ~alpha~ and ~eps~ at
constructor and ~beta~ at ~sample~, and their default values are
~0.6~, ~1e-4~, and ~0.4~, respectively. The detail is described in the
original paper above.



*** Example Usage
#+begin_src python
import numpy as np
from cpprb import PrioritizedReplayBuffer

buffer_size = 256

prb = PrioritizedReplayBuffer(buffer_size,
                              {"obs": {"shape": (4,4)},
                               "act": {"shape": 3},
                               "rew": {},
                               "next_obs": {"shape": (4,4)},
                               "done": {}},
                              alpha=0.5)

for i in range(1000):
    prb.add(obs=np.zeros((4,4)),
            act=np.ones(3),
            rew=0.5,
            next_obs=np.zeros((4,4)),
            done=0)

batch_size = 32
s = prb.sample(batch_size,beta=0.5)

indexes = s["indexes"]
weights = s["weights"]

#  train
#  ...

new_priorities = np.ones_like(weights)
prb.update_priorities(indexes,new_priorities)
#+end_src
*** Notes

*** Technical Detail
To choose prioritized sample efficiently, partial summation and
minimum of pre-calculated weights are stored in Segment Tree data
structure, which is written by C++ and which was an initial main
motivation of this project.

To support multiprocessing, the Segment Tree can be lazily updated,
too.


** DONE Nstep Experience Replay
CLOSED: [2019-11-10 Sun 13:46]
:PROPERTIES:
:EXPORT_FILE_NAME: nstep
:END:
*** Overview

To reduce fluctuation of random sampling effect especially at
bootstrap phase, N-step reward (discounted summation) are useful. By
expanding Bellman equation, a N-step target of Q function becomes
$\sum _{k=0}^{N-1} \gamma ^k r_{t+k} + \gamma ^N \max _{a}
Q(s_{t+N},a)$.

According to [[https://arxiv.org/abs/2007.06700][W. Fedus et. al.]], N-step reward can utilize larger buffer
more effectively. Even though theoretically N-step reward, which is
based on a policy at exploration, is not justified for off-policy, it
still works better.

You can create N-step version replay buffer by specifying ~Nstep~
parameter at constructors of ~ReplayBuffer~ or
~PrioritizedReplayBuffer~. Without modification of its environment,
cpprb summarizes N-step rewards and slides "next" values like ~next_obs~.

~Nstep~ parameter is a ~dict~ with keys of ~"size"~ , ~"rew"~,
~"gamma"~ , and ~"next"~ . ~Nstep["size"]~ is a N-step size and 1-step
is identical with ordinary replay buffer (but
inefficient). ~Nstep["rew"]~, whose type is ~str~ or array-like of
~str~, specifies the (set of) reward(s). ~Nstep["gamma"]~ is a
discount factor for reward summation.  ~Nstep["next"]~ , whose type is
~str~ or array like of ~str~, specifies the (set of) next type
value(s), then ~sample~ method returns (i+N)-th value instead of
(i+1)-th one.


~sample~ also adds ~"discounts"~ (\( \gamma ^{N-1} \)) into returned ~dict~.


Since N-step buffer temporary store the values into local storage, you
need to call ~on_episode_end~ member function at the end of the every
episode end to flush into main storage properly.

*** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{'obs': {"shape": (4,4)},
                      'act': {"shape": 3}
                      'rew': {},
                      'next_obs': {"shape": (4,4)}
                      'done': {}},
                  Nstep={"size": 4,
                         "gamma": 0.99,
                         "rew": "rew",
                         "next": "next_obs"})

for i in range(100):
    done = 1.0 if i%10 == 9 else 0.0
    rb.add(obs=np.zeros((4,4)),
           act=np.ones((3)),
           rew=1.0,
           next_obs=np.zeros((4,4)),
           done=0.0)
    if done:
        rb.on_episode_end()

rb.sample(16)
#+end_src

*** Notes
This N-step feature assumes sequential transitions in a trajectory
(episode) are stored sequentially. If you utilize distributed agent
configuration, you must add a single episode simultaneously.


*** Technical Detail

** DONE Memory Compression
CLOSED: [2019-11-10 Sun 13:33]
:PROPERTIES:
:EXPORT_FILE_NAME: memory_compression
:END:

Since replay buffer stores a large number of data set, memory
efficiency is one of the most important point.

In cpprb, there are two *optional* functionalities named ~next_of~ and
~stack_compress~, which you can turn on manually when constructing
replay buffer.

~next_of~ and ~stack_compress~ can be used together, but currently
none of them are compatible with N-step replay buffer.

These memory compressions rely on the internal memory alignment, so
that these functionalities cannot be used in situations where
sequential steps are not stored sequentially (e.g. distributed
reinforcement learning).

*** ~next_of~

**** Overview
In reinforcement learning, usually a set of observations before and
after a certain action are used for training, so that you save the set
in your replay buffer together. Naively speaking, all observations are
stored twice.

As you know, replay buffer is a ring buffer and the next value should
be stored at the next index, except for the newest edge.

If you specify ~next_of~ argument (whose type is ~str~ or array like
of ~str~), the "next value" of specified values are also created in
the replay buffer automatically and they share the memory location.

The name of the next value adds prefix ~next_~ to the original name
(e.g. ~next_obs~ for ~obs~, ~next_rew~ for ~rew~, and so on).

This functionality has small penalties for manipulating sampled index
and checking the cache for the newest index. (As far as I know, this
penalty is not significant, and you might not notice.)

**** Example Usage
#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

buffer_size = 256

rb = ReplayBuffer(buffer_size,{"obs": {"shape": (84,84)},
                               "act": {"shape": 3},
                               "rew": {},
                               "done": {}}, # You must not specify "next_obs" nor "next_rew".
                  next_of=("obs","rew"))

rb.add(obs=np.ones((84,84)),
       act=np.ones(3),
       next_obs=np.ones((84,84)),
       rew=1,
       next_rew=1,
       done=0)
#+end_src

**** Notes
cpprb does not check the consistence of i-th ~next_foo~ and (i+1)-th
~foo~. This is user responsibility.

Since ~next_foo~ is automatically generated, you must not specify it
in the constructor manually.

**** Technical Detail
Internally, ~next_foo~ is not stored into a ring buffer, but into its chache.
(So still raising error if you don't pass them to ~add~.)

When sampling the ~next_foo~, indices (which is ~numpy.ndarray~) are
shifted (and wraparounded if necessary), then are checked whether they
are on the newest edge of the ring buffer. If the indices are on the
edge, the cached one is extracted.

*** ~stack_compress~

**** Overview
~stack_compress~ is designed for compressing stacked (or sliding
windowed) observation. A famous use case is Atari video game, where 4
frames of display windows are treated as a single observation and the
next observation is the one slided by only 1 frame
(e.g. 1,2,3,4-frames, 2,3,4,5-frames, 3,4,5,6-frames, ...). For this
example, a straight forward approach stores all the frames 4 times.

cpprb with ~stack_compress~ does not store duplicated frames in
stacked observation (except for the end edge of the internal ring
buffer) by utilizing numpy sliding trick.

You can specify ~stack_compress~ parameter, whose type is ~str~ or
array like of ~str~, at constructor.

**** Sample Usage
The following sample code stores ~4~-stacked frames of ~16x16~ data as
a single observation.

#+begin_src python
import numpy as np
from cpprb import ReplayBuffer

rb = ReplayBuffer(32,{"obs":{"shape": (16,16,4)}, 'rew': {}, 'done': {}},
                  next_of = "obs", stack_compress = "obs")

rb.add(obs=(np.ones((16,16,4))),
       next_obs=(np.ones((16,16,4))),
       rew=1,
       done=0)
#+end_src
**** Notes
In order to make compatible with [[https://github.com/openai/gym][OpenAI gym]], the last dimension is
considered as stack dimension (which is not fit to C array memory
order).

For the sake of performance, cpprb does not check the overlapped data
are truly identical, but simply overwrites with new data. Users must
not specify ~stack_compress~ for non-stacked data.

**** Technical Detail
Technically speaking ~numpy.ndarray~ (and other data type supporting
buffer protocol) has properties of item data type, the number of
dimensions, length of each dimension, memory step size of each
dimension, and so on. Usually, no data should overlap memory address,
however, ~stack_compress~ intentionally overlaps the memory addresses
in the stacked dimension.



* Contributing
:PROPERTIES:
:EXPORT_HUGO_SECTION*: contributing
:END:

** DONE Step by Step Merge Request
CLOSED: [2020-01-17 Fri 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: merge_request
:END:

The first step of coding contribution is to fork cpprb on GitLab.com.

The detail steps for fork is described at [[https://docs.gitlab.com/ee/gitlab-basics/fork-project.html][official document]].

After fork cpprb on the web, you can clone repository to your local
machine and set original cpprb as "upstream" by

#+begin_src shell
git clone https://gitlab.com/<Your GitLab Account>/cpprb.git
cd cpprb
git remote add upstream https://gitlab.com/ymd_h/cpprb.git
#+end_src

To make "master" branch clean, you need to create new branch before you edit.

#+begin_src shell
git checkout -b <New Branch Name> master
#+end_src

This process is necessay because "master" and other original branches
might progress during your working.


From here, you can edit codes and make commit as usual.


After finish your work, you must recheck original cpprb and ensure
there is no cnflict.

#+begin_src shell
git pull upstream master
git checkout <Your Branch Name>
git merge master # Fix confliction here!
#+end_src


If everything is fine, you push to your cpprb.

#+begin_src shell
git push origin <Your Branch Name>
#+end_src

Merge request can be created from the web, the detail is described at
[[https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html][official document]].


There is [[https://stackoverflow.com/a/14681796][a good explanation]] for making good Pull Request (merge
request equivalent on GitHub.com)

* DONE Examples
CLOSED: [2020-02-15 Sat 09:23]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: examples
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 800
:END:

** Create ~ReplayBuffer~ for non-simple =gym.Env= with helper functions

#+INCLUDE: "../example/create_buffer_with_helper_func.py" src python

* Comparison
:PROPERTIES:
:EXPORT_HUGO_SECTION*: comparison
:EXPORT_HUGO_WEIGHT: 850
:END:

** DONE Comparison
CLOSED: [2020-02-16 Sun 23:08]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:END:

In this section, we compare cpprb with other replay buffer implementations;

- [[https://github.com/openai/baselines][OpenAI Baselines]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.ReplayBuffer]]
  - [[https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py][baselines.deepq.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/ray-project/ray][Ray RLlib]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/optimizers/replay_buffer.py][ray.rilib.optimizers.replay_buffer.ReplayBuffer]]
  - [[https://github.com/ray-project/ray/blob/master/rllib/optimizers/replay_buffer.py][ray.rllib.optimizers.replay_buffer.PrioritizedReplayBuffer]]
- [[https://github.com/chainer/chainerrl][Chainer ChainerRL]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/replay_buffer.py][chainerrl.replay_buffers.ReplayBuffer]]
  - [[https://github.com/chainer/chainerrl/blob/master/chainerrl/replay_buffers/prioritized.py][chainerrl.replay_buffers.PrioritizedReplayBuffer]]
- [[https://github.com/deepmind/reverb][DeepMind Reverb]]


*Important Notice*

Except cpprb and DeepMind/Reverb, replay buffers are only a part of
their reinforcement learning ecosystem. These libraries don't focus on
providing greatest replay buffers but reinforcement learning.

Our motivation is to provide strong replay buffers to researchers and
developers who not only use existing networks and/or algorithms but
also creating brand-new networks and/or algorithms.

Here, we would like to show that cpprb is enough functional and enough
efficient compared with others.

*** OpenAI Baselines
[[https://github.com/openai/baselines][OpenAI Baselines]] is a set of baseline implementations of reinforcement
learning developed by OpenAI.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.
Using these classes directly is (probably) not expected, but you can
import them like this;

#+begin_src python
from baselines.deepq.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

~ReplayBuffer~ is initialized with ~size~ parameter for replay buffer
size. Additionally, ~PrioritizedReplayBuffer~ requires ~alpha~
parameter for degree of prioritization, too. These parameters doesn't
have default values, so that you need to specify them.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored into replay buffer by calling
~ReplayBuffer.add(self,obs_t,action,reward,obs_tp1,done)~.

For ~PrioritizedReplayBuffer~, the maximum priority at that time is
automatically used for a newly added transition.

These replay buffers are ring buffers, so that the oldest transition
is overwritten by a new transition after the buffer becomes full.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0

rb.add(obs_t,action,reward,obs_tp1,done)
prb.add(obs_t,action,reward,obs_tp1,done) # Store with max. priority
#+end_src

Stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns a tuple of batch size
transition. ~PrioritizedReplayBuffer~ also returns weights and
indexes, too.

#+begin_src python
batch_size = 32
beta = 0.4

obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = prb.sample(batch_size)
#+end_src

Priorities can be updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Ray RLib
[[https://docs.ray.io/en/latest/rllib.html][RLlib]] is reinforcement learning library based on distributed framework
[[https://github.com/ray-project/ray][Ray]].

The source code is published with Apache-2.0 license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~ classes, respectively.

These classes are decorated with ~@DeveloperAPI~, which are intented
to be used by developer when making custom algorithms.

#+begin_src python
from ray.rllib.optimizers.replay_buffer import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

These replay buffer classes initialize like OpenAI Baselines;

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling
~ReplayBuffer.add(self,obs_t,action,reward,obs_tp1,done,weight)~ and
~PrioritizedReplayBuffer.add(self,obs_t,action,reward,obs_tp1,done,weight)~.

In RLlib, ~PrioritizedReplayBuffer~ can take ~weight~ parameter to
specify priority at the same time. Brcause of unified API,
~ReplayBuffer~ also requires ~weight~ parameter, even though it is not
used at all. Moreover, the ~weight~ does not have default parameter
(such as ~None~), you need to pass something.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = 0.0
weight = 0.5

rb.add(obs_t,action,reward,obs_tp1,done,None)
prb.add(obs_t,action,reward,obs_tp1,done,weight)
#+end_src


Like OpenAI Baselines, stored transitions can be sampled by calling
~ReplayBuffer.sample(self,batch_size)~ or
~PrioritizedReplayBuffer.sample(self,batch_size,beta)~.

~ReplayBuffer~ returns a tuple of batch size
transition. ~PrioritizedReplayBuffer~ also returns weights and
indexes, too.

#+begin_src python
batch_size = 32
beta = 0.4

obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = rb.sample(batch_size)
obs_batch, act_batch, rew_batch, next_obs_batch, done_mask, weights, idxes = prb.sample(batch_size)
#+end_src

Priorities can be also updated by calling
~PrioritizedReplayBuffer.update_priorities(self,idxes,priorities)~,
too.

#+begin_src python
prb.update_priorities(idxes,priorities)
#+end_src

Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full.

*** Chainer ChainerRL
[[https://github.com/chainer/chainerrl][ChainerRL]] is a deep reinforcement learning library based on a
framework [[https://github.com/chainer/chainer][Chainer]]. Chainer (including ChainerRL) has already stopped
active development, and development team (Preferred Networks) joined
to [[https://pytorch.org/][PyTorch]] development.

The source code is published with MIT license.

Ordinary and prioritized experience replay are implemented with
~ReplayBuffer~ and ~PrioritizedReplayBuffer~, respectively.

#+begin_src python
from chainerrl.replay_buffers import ReplayBuffer, PrioritizedReplayBuffer
#+end_src

ChainerRL has slighly different API from OpenAI Baselines' one.

~ReplayBuffer~ is initialized with ~capacity=None~ parameter for
buffer size and ~num_steps=1~ parameter for Nstep configuration.

~PrioritizedReplayBuffer~ can additionally take parameters of
~alpha=0.6~, ~beta0=0.4~, ~betasteps=2e5~, ~eps=0.01~,
~normalize_by_max=True~, ~error_min=0~, ~error_max=1~.

In ChainerRL, beta (correction for weight) parameter starts from
~beta0~, automatically increases with equal step size during
~betasteps~ time iterations and after that becomes ~1.0~.

#+begin_src python
buffer_size = int(1e6)
alpha = 0.6

rb = ReplayBuffer(buffer_size)
prb = PrioritizedReplayBuffer(buffer_size,alpha)
#+end_src

A transition is stored by calling
~ReplayBuffer.append(self,state,action,reward,next_state=None,next_action=None,is_state_terminate=False,env_id=0,**kwargs)~.
Additional keyward arguments are stored, too, so that you can use any
custom environment values. By specifying ~env_id~, multiple trajectory
can be tracked with Nstep configuration.

#+begin_src python
obs_t = [0, 0, 0]
action = [1]
reward = 0.5
obs_tp1 = [1, 1, 1]
done = False

rb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
prb.add(obs_t,action,reward,obs_tp1,is_state_terminal=done)
#+end_src

Stored transitions are sampled by calling
~ReplayBuffer.sample(self,num_experience)~ and
~PrioritizedReplayBuffer.sample(self,n)~.

Apart from other implementations, ChainerRL's replay buffers return
unique (non-duplicated) transitions, so that batch_size must be
smaller than stored transition size. Furthermore, they return a Python
~list~ of a ~dict~ of transition instead of a Python ~tuple~ of
environment values.

#+begin_src python
batch_size = 32

# Need additional modification to take apart
transitions = rb.sample(batch_size)
transitions_with_weight = prb.sample(batch_size)
#+end_src

Update index cannot be specified manually, but
~PrioritizedReplayBuffer~ memorizes sampled indexes.
(Without ~sample~, user cannot update priority.)

#+begin_src python
prb.update_priorities(priorities)
#+end_src


Internally, these replay buffers utilize Python ~list~ for storage, so
that the memory usage gradually increase until the buffer becomes
full. In ChainerRL, storage is not a simple Python ~list~, but two
~list~ to pop out the oldest element with O(1) time.

*** DeepMind Reverb
[[https://github.com/deepmind/reverb][Reverb]] is relatively new, which was released on 26th May 2020 by
DeepMind.

Reverb is a framework for experience replay like cpprb. By utilizing
server-client model, Reverb is mainly optimized for large-scale
distributed reinforcement learning.

The source code is published with Apache-2.0 license.

Currently (28th June 2020), Reverb officially says it is still
non-production level and requries a development version of TensorFlow
(i.e. tf-nightly 2.3.0.dev20200604).

Ordinary and prioritized experience replay are constructed by setting
~reverb.selectors.Uniform()~ and ~reverb.selectors.Prioritized(alpha)~
to ~sampler~ argument in ~reverb.Table~ constructor, respectively.

Following sample code constructs a server with two replay buffers
listening a port ~8000~.

#+begin_src python
import reverb

buffer_size = int(1e6)
alpha = 0.6

server = reverb.Server(tables=[reverb.Table(name="ReplayBuffer",
                                            sampler=reverb.selectors.Uniform(),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size),
                               reverb.Table(name="PrioritizedReplayBuffer",
                                            sampler=reverb.selectors.Prioritized(alpha),
                                            remover=reverb.selectors.Fifo(),
                                            rate_limiter=reverb.rate_limiters.MinSize(1),
                                            max_size=buffer_size)],
                       port=8000)
#+end_src

By changing ~selector~ and ~remover~, we can use different algorithms
for sampling and overwriting, respectively.

Supported algorithms implemented at ~reverb.selectors~ are following;

- ~Uniform~: Select uniformly.
- ~Prioritized~: Select proportional to stored priorities.
- ~Fifo~: Select oldest data.
- ~Lifo~: Select newest data.
- ~MinHeap~: Select data with lowest priority.
- ~MaxHeap~: Select data with highest priority.


There are 3 ways to store a transition.

The first method uses ~reverb.Client.insert~. Not only prioritized
replay buffer but also ordinary replay buffer requires priority even
though it is not used.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

client.insert([obs_t,action,reward,obs_tp1,done],priorities={"ReplayBuffer":1.0})
client.insert([obs_t,action,reward,obs_tp1,done],priorities={"PrioritizedReplayBuffer":1.0})
#+end_src

The second method uses ~reverb.Client.writer~, which is internally
used in ~reverb.Client.insert~, too. This method can be more efficient
because you can flush multiple items together by calling
~reverb.Writer.close~ insead of one by one.

#+begin_src python
import reverb

client = reverb.Client(f"localhost:{server.port}")

obs_t = [0, 0, 0]
action = [1]
reward = [0.5]
obs_tp1 = [1, 1, 1]
done = [0]

with client.writer(max_sequence_length=1) as writer:
    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="ReplayBuffer",num_timesteps=1,priority=1.0)

    writer.append([obs_t,action,reward,obs_tp1,done])
    writer.create_item(table="PrioritizedReplayBuffer",num_timesteps=1,priority=1.0)
#+end_src

The last method uses ~reverb.TFClient.insert~. This class is designed
to be used in TensorFlow graph.

#+begin_src python
import reverb

tf_client = reverb.TFClient(f"localhost:{server.port}")

obs_t = tf.constant([0, 0, 0])
action = tf.constant([1])
reward = tf.constant([0.5])
obs_tp1 = tf.constant([1, 1, 1])
done = tf.constant([0])

tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["ReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
tf_client.insert([obs_t,action,reward,obs_tp1,done],
                 tables=tf.constant(["PrioritizedReplayBuffer"]),
                 priorities=tf.constant([1.0],dtype=tf.float64))
#+end_src

~tables~ parameter must be ~tf.Tensor~ of ~str~ with rank 1 and
~priorities~ parameter must be ~tf.Tensor~ of ~float64~ with
rank 1. The lengths of ~tables~ and ~priorities~ must match.


Sampling transitions can be realized by 3 ways, too.

The first method utilizes ~reverb.Client.sample~, which returns
generator of ~reverb.replay_sample.ReplaySample~. As long as we can
investigate, beta-parameter is not supported and weight is not
calculated at prioritized experience replay.

#+begin_src python
batch_size = 32

transitions = client.sample("ReplayBuffer",num_samples=batch_size)
transitions_with_priority = client.sample("PrioritizedReplayBuffer",num_samples=batch_size)
#+end_src


The second method uses ~reverb.TFClient.sample~, which does not
support batch sampling.

#+begin_src python
transition = tf_client.sample("ReplayBuffer",
                              [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
transition_priority = tf_client.sample("PrioritizedReplayBuffer",
                                       [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64])
#+end_src


The last method is completely different from others, which calls
~reverb.TFClient.dataset~, returning ~reverb.ReplayDataset~ derived
from ~tf.data.Dataset~.

Once creating ~ReplayDataset~, the dataset can be used as generator
and automatically fetches transitions from its replay buffer with
proper timing.


#+begin_src python
dataset = tf_client.dataset("ReplayBuffer",
                            [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                            [4,1,1,4,1])
dataset_priority = tf_client.dataset("PrioritizedReplayBuffer",
                                     [tf.float64,tf.float64,tf.float64,tf.float64,tf.float64],
                                     [4,1,1,4,1])
#+end_src


Priorities can be updated by ~reverb.Client.mutate_priorities~ or
~reverb.TFClient.update_priorities~. Aside from other implementations,
key is not integer sequence but hash, so that the key must be taken
from sampled items by accsessing ~ReplaySample.info.key~.

#+begin_src python
for t in transitions_with_priority:
    client.mutate_priorities("PriorotizedReplayBuffer",updates={t.info.key: 0.5})

tf_client.update_priorities("PrioritizedReplayBuffer",
                            transition_priority.info.key,
                            priorities=tf.constant([0.5],dtype=tf.float64)
#+end_src

*** Other Implementations
There are also some other replay buffer implementations, which we
couldn't review well. In future, we would like to investigate these
implementations and compare with cpprb.

- [[https://github.com/tensorflow/agents][TF-Agents]] :: TensorFlow official reinforcement learning library
- [[https://github.com/google-research/seed_rl][SEED RL]] :: Scalable and Effificient Deep-RL
- [[https://github.com/tensorflow/models][TensorFlow Model Garden]] :: TensorFlow example implementation


** DONE Functionality
CLOSED: [2020-02-24 Mon 12:49]
:PROPERTIES:
:EXPORT_FILE_NAME: functionality
:EXPORT_HUGO_WEIGHT: 10
:END:

The following table summarizes functionalities of replay buffers.

|                           | cpprb                     | OpenAI/Baselines                          | Ray/RLlib                                                  | Chainer/ChainerRL          | DeepMind/Reverb |
|---------------------------+---------------------------+-------------------------------------------+------------------------------------------------------------+----------------------------+-----------------|
| *Flexible Environment*    | Yes                       | No                                        | No                                                         | Yes                        | Yes             |
| *Nstep*                   | Yes                       | No                                        | Yes                                                        | Yes                        | No              |
| *Parellel Exploration*    | No ([[https://gitlab.com/ymd_h/cpprb/-/milestones/3][milestone]], [[https://gitlab.com/ymd_h/cpprb/-/milestones/9][milestone]]) | Yes (Avarages gradients of MPI processes) | Yes (Concatenates sample batches from distributed buffers) | No                         | Yes             |
| *Save/Load*               | No (Cannot =pickle=)      | No (Maybe can =pickle=)                   | No (Maybe can =pickle=. Trained policies can save/load.)   | Yes                        | Yes             |
| *Deep Learning Framework* | Anything                  | TensorFlow 1.14 (only this version)       | Anything (Helper functions for [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow][TensorFlow]] and [[https://ray.readthedocs.io/en/latest/rllib-concepts.html#building-policies-in-pytorch][PyTorch]])     | [[https://chainer.org/][Chainer]] ([[https://chainer.org/announcement/2019/12/05/released-v7.html][maintenance only]]) | TensorFlow 2.3  |


** DONE Benchmark
CLOSED: [2020-02-16 Sun 23:24]
:PROPERTIES:
:EXPORT_HUGO_WEIGHT: 20
:EXPORT_FILE_NAME: benchmark
:END:

Because of dependant TensorFlow version incompatibility, two set of
benchmarks are executed.

*** Benchmark 1
The first benchmark compares OpenAI/Baselines, Ray/RLlib,
Chainer/ChainerRL, and cpprb.

**** Settings

We use following docker image to take benchmarks;

#+INCLUDE: "../benchmark/Dockerfile" src dockerfile

- OpenAI Baselines requires TensorFlow 1.14
- OpenAI Baselines at PyPI seems to be obsolete and requires non-free MuJoCo.
- RLlib requres Pandas, too.


The benchmark script is as follows;

#+INCLUDE: "../benchmark/benchmark.py" src python


**** Results
[[/cpprb/benchmark/ReplayBuffer_add.png]]

[[/cpprb/benchmark/ReplayBuffer_sample.png]]

[[/cpprb/benchmark/PrioritizedReplayBuffer_add.png]]


[[/cpprb/benchmark/PrioritizedReplayBuffer_sample.png]]


*** Benchmark 2
The second benchmark compares DeepMind/Reverb and cpprb.
Reverb has multiple ways of adding and sampling.

**** Settings
We use following docker image to take benchmarks;

#+INCLUDE: "../benchmark2/Dockerfile" src dockerfile

- DeepMind/Reverb requires development version TensorFlow 2.3.0.dev20200604

The benchmark script is as follows;

#+INCLUDE: "../benchmark2/benchmark.py" src python

**** Results
[[/cpprb/benchmark/ReplayBuffer_add2.png]]

[[/cpprb/benchmark/ReplayBuffer_sample2.png]]

[[/cpprb/benchmark/PrioritizedReplayBuffer_add2.png]]


[[/cpprb/benchmark/PrioritizedReplayBuffer_sample2.png]]

* DONE Misc
CLOSED: [2020-01-17 Fri 22:31]
:PROPERTIES:
:EXPORT_HUGO_SECTION*: misc
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_WEIGHT: 999
:END:

In this section, cpprb related miscellaneous information are described.

* DONE FAQ
CLOSED: [2020-06-06 Sat 13:50]
:PROPERTIES:
:EXPORT_FILE_NAME: _index
:EXPORT_HUGO_SECTION*: faq
:EXPORT_HUGO_WEIGHT: 900
:END:

** Can I save/load ReplayBuffer?

No. Unfortunately, save/load full ~ReplayBuffer~ object state is not
supported.

However, you can get all the stored transisions (without "priority" in
~PrioritizedReplayBuffer~) by calling
~ReplayBuffer.get_all_transitions()~ and a set of transitions can be
stored at the same time by calling ~ReplayBuffer.add(**kwargs)~.

** "IlIegal instruction (core dumped)" when "import cpprb"

Pre-built binary does not match with your environment. Please
re-install from source code.

#+begin_src shell
pip uninstall cpprb
pip install cpprb --no-binary
#+end_src

This installation requires sufficient C++ compiler. (see [[https://ymd_h.gitlab.io/cpprb/installation/][Installation]])
